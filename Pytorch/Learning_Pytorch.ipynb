{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca000268-b758-463f-8614-789966470ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77b3423-c231-4704-91cc-bc879d8a2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4639436e-0e49-46dd-8a90-2c3c2276658b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6d3367-e6a1-42a4-b716-f2a9a80b3475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17ec230-0a77-4b69-aca8-562109a0e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d0c32f-f502-4c3c-a0d3-e84575a8b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100S-PCIE-32GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ece2c5-4f92-4543-8b6e-b973eb491988",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe99e7d7-0e2f-4d24-9b33-34b53cd1e848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor of ones\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b448d13f-538e-48a2-b24d-2f4254f0ae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53075bd9-b401-4763-b573-2efe105eb9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec8e07d-2cb6-4b1e-896f-225748924ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0ecaae-dead-4866-975f-c9278cfdbe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26eef727-7842-4bd1-a555-0cfaad4cfe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider a list of coordinates\n",
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3df69b6-2176-47df-ad2e-1ad7c058a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e319b743-37ed-450d-90e8-f784da0082d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor(1.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0], points[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5ac9b0-7f7e-4ee5-a4b4-b6b71856e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3078cde3-fa0f-4f43-9c64-b569656c464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8474478e-2d7f-4646-a60d-0db90186185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better representation will be\n",
    "points_alt = torch.tensor([[4.0,1.0], [5,3], [2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c3da2d0-e50b-4496-bbfb-962f7f90819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47127da3-2e38-4227-be29-64c4f1ce645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can access points with indexing\n",
    "points_alt[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c57cd90-2844-4f03-a1d6-065dcaac6756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points_alt[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee532d39-b8f5-4926-a5b8-d7e7c28f5cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2d066d1-16d1-404d-a5bd-005a7cfb7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the below doesn't work in this case as now we have multiple values\n",
    "# float(points_alt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a240ee5d-0f3b-4852-b7a8-d7a733e1cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f0c77be-304b-4d62-98d9-aa32097a6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want a matrix of zeros of some specific shape\n",
    "points_0 = torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a741256f-dd95-4f30-882d-d6603aed78a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fae356c-bf71-4762-9410-62279ba293f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41522c7-e415-4362-88e2-346e8b9dd84a",
   "metadata": {},
   "source": [
    "##### Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7c69e6b-992e-4aa8-bd3a-5fc16950d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab5d345a-3752-4451-9394-fb40f97fa720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch tensors can be indexed similar to python lists\n",
    "points_alt[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691852b3-d288-438d-ac47-c005e069bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5f4010b-c026-4ddb-95a5-67ccb8fd3bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "504fd02f-96d0-4569-b5aa-bada93a153b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None] adds another dimesion\n",
    "points_alt_none = points_alt[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2c9e57-1e0e-4e23-a112-6ab1e507d5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "423764ee-cb95-4c54-b5a5-7ce9bd194956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt_none.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40d21047-d244-4a0e-be96-d79cc398b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch also has something called advanced indexing, which will come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b4493bf-22b9-4d84-936f-6942595c7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think pytorch does channels, rows, cols? This is different from tf, and how we naturally see an image\n",
    "# when working with PyTorch, we reorder the dimensions of image data to fit its convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc309a-6297-4e5e-b2bf-b5f71bc35a2b",
   "metadata": {},
   "source": [
    "##### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5664324-ccb9-4e39-a01a-7071421a7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # [channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7765c4-a043-4470-ac78-65198e6a9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "543a49e9-3619-4eaa-80fc-43b6fd75c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9226382b-9de0-460c-ad6d-2badd00b5b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd0581be-74bd-464a-8a54-5fc95bfdee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] + weights[1] + weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce991aa0-a8b9-47cc-a57d-748b8be45144",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # [batch_size, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c972f8e2-e650-4951-98a2-f769750b4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel dimension is either 0, or 1. But from backwords it's always -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0167148-df7c-4f22-9a49-60a1331a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unweighted means - Think we are trying to convert an RGB to gray scale\n",
    "img_gray_naive = img_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e458089-1f17-4ca7-81f9-d09ed4304f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8001,  0.8877, -0.1797, -0.7456,  0.3516],\n",
       "        [-0.6687,  0.3843, -0.2431,  0.2314, -0.2358],\n",
       "        [ 0.3426, -0.0952,  0.1261,  0.9040,  0.0551],\n",
       "        [ 0.0951, -0.0654,  0.3623,  0.0068, -0.4677],\n",
       "        [ 0.2563, -0.0020,  0.4648,  0.1191, -1.1409]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8acfb48c-e7f8-4c61-bb63-9c1441f17f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0047,  0.4983,  0.3472, -0.4876, -0.3106],\n",
       "         [-1.0144,  1.7442, -0.7993,  0.3430,  1.0077],\n",
       "         [ 0.1687,  0.2100,  1.2146, -0.2802, -0.1263],\n",
       "         [-0.1062, -0.2323,  0.4342, -0.2162,  0.1333],\n",
       "         [ 1.5472, -0.9745,  0.1066, -0.6785, -1.5719]],\n",
       "\n",
       "        [[-0.4684,  0.7209, -0.9660, -1.2417,  0.3014],\n",
       "         [-0.7863,  0.0165,  0.7355, -1.2805, -0.6744],\n",
       "         [ 0.5528,  0.8260, -0.7047,  1.2193, -0.5847],\n",
       "         [ 0.0437,  0.3597,  0.5756, -0.0983, -0.8290],\n",
       "         [ 0.3528, -1.0096,  1.0047,  0.3288, -1.1760]],\n",
       "\n",
       "        [[-1.9366,  1.4440,  0.0799, -0.5075,  1.0640],\n",
       "         [-0.2053, -0.6080, -0.6654,  1.6317, -1.0405],\n",
       "         [ 0.3064, -1.3216, -0.1315,  1.7728,  0.8762],\n",
       "         [ 0.3479, -0.3235,  0.0773,  0.3350, -0.7075],\n",
       "         [-1.1310,  1.9779,  0.2829,  0.7072, -0.6749]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d38732c4-e6e6-4a88-89af-270d7a5f0d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2265"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.3847 + 0.6229 - 0.9177)/3 # = -0.2265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d6d73fb-a5ae-41e4-bcbc-d9a9bd9f2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "249b1a94-7825-4b40-960f-eeca63e54f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6875,  0.0469,  1.4429,  0.0663, -0.1364],\n",
       "         [-0.8015, -0.5903, -0.0325,  0.3303, -0.1104],\n",
       "         [ 0.7650,  0.0280,  0.3556,  0.0189,  0.3670],\n",
       "         [-0.1909,  0.4168, -0.0826, -1.6575,  0.8449],\n",
       "         [-0.1974,  0.7443, -0.0904,  0.1640, -0.6578]],\n",
       "\n",
       "        [[ 1.0328,  0.1965, -1.0914, -1.2866,  0.5131],\n",
       "         [-1.0083, -0.1140, -0.3278,  0.0581,  0.1904],\n",
       "         [-0.1528,  0.5780, -0.1733,  0.1035, -0.7069],\n",
       "         [ 0.1528, -0.0887,  0.2454,  0.3072, -0.1545],\n",
       "         [ 0.6126, -0.5900, -0.4092, -0.1860, -0.8718]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fa65bf5-e85f-44a3-b6b9-4793fc5767b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2eb4b6d-6070-43ae-9cbd-b946dcd22fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also have the gray scale weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22ee1775-51ee-4d47-b810-bc44a8825899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80e15086-68fd-4ca4-bc8c-5c5e90d16248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63b954d6-7221-4535-9db4-f338c58b2329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(-1).unsqueeze_(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c4ef2df-8629-44d6-aa2b-f3d5c5f63f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2126]],\n",
       "\n",
       "        [[0.7152]],\n",
       "\n",
       "        [[0.0722]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4875a8-2fb0-4f75-b31b-b38a91f5055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast the weghts so that they can be multiplied with the random tensors\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b611af27-f946-4ccd-985a-a800c60eb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bafd310a-3fa4-4ddb-b2bb-21ae66c5cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights.shape, batch_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8ef1745-d92b-46d5-8387-7dac0d28d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56967141-d945-465d-8a73-6cca47e48a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted.shape, batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e08fd1b8-cb8d-44ef-8034-83ab2bfc2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do the same with the einsum function\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c150b310-e6a3-43ce-9eb4-c2f5de62f0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy.shape, batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b4bcdec-a15a-40b9-b3bb-d003c5574318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_gray_weighted == img_gray_weighted_fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2c973f8-25d2-4193-949f-19c30d08454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted == batch_gray_weighted_fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b41d60df-b3a9-4a9c-81f8-c69ed272e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a lot of book keeping, therefore practitioners decided to give names to the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c190b90-11af-4872-8aab-c0e0059d6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names = ['channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2fa3e45-7f77-4823-a2ab-8c939eadedab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82e6cf40-515a-40e7-88fd-2b8196f580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can add names to an existing tensor - with refine_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f37c619e-eee0-44e8-8b51-30b575bce44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95dfb51d-4538-484a-9a23-fdc43bd513ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named.shape, img_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "077f3c6d-e77b-444e-a406-d6082d5dea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can rename or drop existing names with rename method too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22fbe963-4327-4cb1-9a53-fcdc18d5cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named_renamed = img_named.rename(..., \"ch\", 'r', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d87fecc4-111b-42af-adcd-74a627645bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ch', 'r', 'c')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named_renamed.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5d829ea-a78a-4a77-9bb4-1993ed6ae7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when operations involve tow such tensors now pytorch will also look into the names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ccd090f-c2ec-4c0c-b719-1a61078c32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The book only uses unnamed tensors though - as it is easier even when working outside of tensors without tha capability of naming them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5322b4-2c75-410d-9608-b7338d42bfe6",
   "metadata": {},
   "source": [
    "Storage of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a072a8a2-39eb-4e99-9189-8443d4abe42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[4.0,1.0], [5.0, 3.0], [2.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8540ae9-1d90-4d76-87c4-759b688b1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_point = points[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8fcedd9e-9739-4698-a154-11ad555a97bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "589836f9-ce70-4c0f-8a6c-1f93530219fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c1330f4-f666-4173-a382-6e5bb985fd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66885f57-3d14-4d04-959b-89e890cf9305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae1837-2c9a-44fe-a51c-3e9faf54e5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaede4b-611f-4775-a4f4-0707e083205a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU 2.5 (py312)",
   "language": "python",
   "name": "pytorch-gpu-2.5-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
