{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca000268-b758-463f-8614-789966470ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77b3423-c231-4704-91cc-bc879d8a2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we on GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4639436e-0e49-46dd-8a90-2c3c2276658b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6d3367-e6a1-42a4-b716-f2a9a80b3475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17ec230-0a77-4b69-aca8-562109a0e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d0c32f-f502-4c3c-a0d3-e84575a8b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100S-PCIE-32GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ece2c5-4f92-4543-8b6e-b973eb491988",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe99e7d7-0e2f-4d24-9b33-34b53cd1e848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor of ones\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b448d13f-538e-48a2-b24d-2f4254f0ae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53075bd9-b401-4763-b573-2efe105eb9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec8e07d-2cb6-4b1e-896f-225748924ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0ecaae-dead-4866-975f-c9278cfdbe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26eef727-7842-4bd1-a555-0cfaad4cfe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider a list of coordinates\n",
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3df69b6-2176-47df-ad2e-1ad7c058a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e319b743-37ed-450d-90e8-f784da0082d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor(1.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0], points[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5ac9b0-7f7e-4ee5-a4b4-b6b71856e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3078cde3-fa0f-4f43-9c64-b569656c464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8474478e-2d7f-4646-a60d-0db90186185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better representation will be\n",
    "points_alt = torch.tensor([[4.0,1.0], [5,3], [2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c3da2d0-e50b-4496-bbfb-962f7f90819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47127da3-2e38-4227-be29-64c4f1ce645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can access points with indexing\n",
    "points_alt[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c57cd90-2844-4f03-a1d6-065dcaac6756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points_alt[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee532d39-b8f5-4926-a5b8-d7e7c28f5cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2d066d1-16d1-404d-a5bd-005a7cfb7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the below doesn't work in this case as now we have multiple values\n",
    "# float(points_alt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a240ee5d-0f3b-4852-b7a8-d7a733e1cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f0c77be-304b-4d62-98d9-aa32097a6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want a matrix of zeros of some specific shape\n",
    "points_0 = torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a741256f-dd95-4f30-882d-d6603aed78a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fae356c-bf71-4762-9410-62279ba293f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41522c7-e415-4362-88e2-346e8b9dd84a",
   "metadata": {},
   "source": [
    "##### Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7c69e6b-992e-4aa8-bd3a-5fc16950d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab5d345a-3752-4451-9394-fb40f97fa720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch tensors can be indexed similar to python lists\n",
    "points_alt[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691852b3-d288-438d-ac47-c005e069bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5f4010b-c026-4ddb-95a5-67ccb8fd3bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "504fd02f-96d0-4569-b5aa-bada93a153b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None] adds another dimesion\n",
    "points_alt_none = points_alt[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2c9e57-1e0e-4e23-a112-6ab1e507d5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "423764ee-cb95-4c54-b5a5-7ce9bd194956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_alt_none.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40d21047-d244-4a0e-be96-d79cc398b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch also has something called advanced indexing, which will come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b4493bf-22b9-4d84-936f-6942595c7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think pytorch does channels, rows, cols? This is different from tf, and how we naturally see an image\n",
    "# when working with PyTorch, we reorder the dimensions of image data to fit its convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc309a-6297-4e5e-b2bf-b5f71bc35a2b",
   "metadata": {},
   "source": [
    "##### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5664324-ccb9-4e39-a01a-7071421a7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # [channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7765c4-a043-4470-ac78-65198e6a9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "543a49e9-3619-4eaa-80fc-43b6fd75c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9226382b-9de0-460c-ad6d-2badd00b5b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd0581be-74bd-464a-8a54-5fc95bfdee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] + weights[1] + weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce991aa0-a8b9-47cc-a57d-748b8be45144",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # [batch_size, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c972f8e2-e650-4951-98a2-f769750b4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel dimension is either 0, or 1. But from backwords it's always -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0167148-df7c-4f22-9a49-60a1331a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unweighted means - Think we are trying to convert an RGB to gray scale\n",
    "img_gray_naive = img_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e458089-1f17-4ca7-81f9-d09ed4304f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7445,  0.2337, -0.2065,  0.1828, -0.0991],\n",
       "        [-0.2559,  1.0023,  0.4916, -0.5487, -0.2703],\n",
       "        [ 0.1969,  1.0087,  0.7151, -0.9006, -0.5158],\n",
       "        [-0.0405, -1.0228, -0.0404,  0.2334,  0.5461],\n",
       "        [-0.1886, -0.1697,  0.8956,  0.5965,  0.7253]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8acfb48c-e7f8-4c61-bb63-9c1441f17f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1790, -0.6444, -0.3857,  0.3962, -0.2896],\n",
       "         [ 0.8772,  0.8874,  0.6478,  0.5622,  0.3327],\n",
       "         [-0.0298,  0.4361,  0.4698, -0.9689,  0.5550],\n",
       "         [ 0.6249, -1.9649, -1.0084, -1.1850,  1.5948],\n",
       "         [-0.7070,  0.2092,  1.2222,  0.0439,  0.2546]],\n",
       "\n",
       "        [[ 1.4887,  0.6823, -0.6305,  1.3930,  0.6485],\n",
       "         [-1.6053,  1.3864, -0.1545, -1.5221, -1.3154],\n",
       "         [ 0.6864,  0.6710,  0.9948, -1.1563, -1.0468],\n",
       "         [-0.4547, -0.5329,  0.2094,  1.1279, -0.1734],\n",
       "         [ 0.3327, -0.9253,  0.3268,  1.3353,  1.6400]],\n",
       "\n",
       "        [[-0.4341,  0.6632,  0.3967, -1.2408, -0.6563],\n",
       "         [-0.0397,  0.7329,  0.9816, -0.6861,  0.1717],\n",
       "         [-0.0660,  1.9189,  0.6808, -0.5767, -1.0556],\n",
       "         [-0.2917, -0.5707,  0.6778,  0.7572,  0.2170],\n",
       "         [-0.1915,  0.2069,  1.1377,  0.4103,  0.2814]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d38732c4-e6e6-4a88-89af-270d7a5f0d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2265"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.3847 + 0.6229 - 0.9177)/3 # = -0.2265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d6d73fb-a5ae-41e4-bcbc-d9a9bd9f2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "249b1a94-7825-4b40-960f-eeca63e54f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2491,  0.6555, -0.1823, -0.0999,  0.7110],\n",
       "         [-0.3447, -1.6559,  0.6526, -0.4716,  1.0555],\n",
       "         [-0.5658,  0.4449,  0.3847,  0.1282,  0.4777],\n",
       "         [ 1.3819, -0.3143, -0.2909,  0.1491,  1.2747],\n",
       "         [-1.4858, -0.6571,  0.0560,  0.0923,  0.2952]],\n",
       "\n",
       "        [[-0.2949,  0.4608, -0.1266, -1.0328,  0.7191],\n",
       "         [-0.2214,  0.1037,  0.4005, -0.2200,  0.3071],\n",
       "         [ 0.8408, -0.5630,  0.2705,  0.1536,  0.0094],\n",
       "         [-0.1787, -0.5849,  0.4972, -0.0766, -0.6492],\n",
       "         [ 0.5103,  0.2006, -0.0172, -0.3173,  0.6682]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fa65bf5-e85f-44a3-b6b9-4793fc5767b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2eb4b6d-6070-43ae-9cbd-b946dcd22fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also have the gray scale weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22ee1775-51ee-4d47-b810-bc44a8825899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80e15086-68fd-4ca4-bc8c-5c5e90d16248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63b954d6-7221-4535-9db4-f338c58b2329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(-1).unsqueeze_(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c4ef2df-8629-44d6-aa2b-f3d5c5f63f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2126]],\n",
       "\n",
       "        [[0.7152]],\n",
       "\n",
       "        [[0.0722]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4875a8-2fb0-4f75-b31b-b38a91f5055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast the weghts so that they can be multiplied with the random tensors\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b611af27-f946-4ccd-985a-a800c60eb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bafd310a-3fa4-4ddb-b2bb-21ae66c5cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights.shape, batch_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8ef1745-d92b-46d5-8387-7dac0d28d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56967141-d945-465d-8a73-6cca47e48a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted.shape, batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e08fd1b8-cb8d-44ef-8034-83ab2bfc2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do the same with the einsum function\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c150b310-e6a3-43ce-9eb4-c2f5de62f0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy.shape, batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b4bcdec-a15a-40b9-b3bb-d003c5574318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_gray_weighted == img_gray_weighted_fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2c973f8-25d2-4193-949f-19c30d08454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted == batch_gray_weighted_fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b41d60df-b3a9-4a9c-81f8-c69ed272e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a lot of book keeping, therefore practitioners decided to give names to the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c190b90-11af-4872-8aab-c0e0059d6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names = ['channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2fa3e45-7f77-4823-a2ab-8c939eadedab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82e6cf40-515a-40e7-88fd-2b8196f580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can add names to an existing tensor - with refine_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f37c619e-eee0-44e8-8b51-30b575bce44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95dfb51d-4538-484a-9a23-fdc43bd513ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named.shape, img_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "077f3c6d-e77b-444e-a406-d6082d5dea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can rename or drop existing names with rename method too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22fbe963-4327-4cb1-9a53-fcdc18d5cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named_renamed = img_named.rename(..., \"ch\", 'r', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d87fecc4-111b-42af-adcd-74a627645bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ch', 'r', 'c')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named_renamed.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5d829ea-a78a-4a77-9bb4-1993ed6ae7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when operations involve tow such tensors now pytorch will also look into the names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ccd090f-c2ec-4c0c-b719-1a61078c32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The book only uses unnamed tensors though - as it is easier even when working outside of tensors without tha capability of naming them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5322b4-2c75-410d-9608-b7338d42bfe6",
   "metadata": {},
   "source": [
    "Storage of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a072a8a2-39eb-4e99-9189-8443d4abe42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[4.0,1.0], [5.0, 3.0], [2.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8540ae9-1d90-4d76-87c4-759b688b1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_point = points[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8fcedd9e-9739-4698-a154-11ad555a97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_point.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb77007e-39d9-4de0-b85b-b719b2a25fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "589836f9-ce70-4c0f-8a6c-1f93530219fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c1330f4-f666-4173-a382-6e5bb985fd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66885f57-3d14-4d04-959b-89e890cf9305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0aaede4b-611f-4775-a4f4-0707e083205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_t = points.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dcc07a9d-9668-4270-b550-9100e6f1a9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4f2de3d-de3f-4d91-9bb6-504de3ba5a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_t.stride()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU 2.5 (py312)",
   "language": "python",
   "name": "pytorch-gpu-2.5-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
